{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec之Skip-Gram模型\n",
    "\n",
    "下面代码将用TensorFlow实现Word2Vec中的Skip-Gram模型。\n",
    "\n",
    "关于Skip-Gram模型请参考上一篇[知乎专栏文章](https://zhuanlan.zhihu.com/p/27234078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 加载数据\n",
    "\n",
    "数据集使用的是来自Matt Mahoney的维基百科文章，数据集已经被清洗过，去除了特殊符号等，并不是全量数据，只是部分数据，所以实际上最后训练出的结果很一般（语料不够）。\n",
    "\n",
    "如果想获取更全的语料数据，可以访问以下网站，这是gensim中Word2Vec提供的语料：\n",
    "\n",
    "- 来自Matt Mahoney预处理后的[文本子集](http://mattmahoney.net/dc/enwik9.zip)，里面包含了10亿个字符。\n",
    "- 与第一条一样的经过预处理的[文本数据](http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2)，但是包含了30个亿的字符。\n",
    "- 多种语言的[训练文本](http://www.statmt.org/wmt11/translation-task.html#download)。\n",
    "- [UMBC webbase corpus](http://ebiquity.umbc.edu/redirect/to/resource/id/351/UMBC-webbase-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/text8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 数据预处理\n",
    "\n",
    "数据预处理过程主要包括：\n",
    "\n",
    "- 替换文本中特殊符号并去除低频词\n",
    "- 对文本分词\n",
    "- 构建语料\n",
    "- 单词映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来完成数据的预处理\n",
    "def preprocess(text, freq=5):\n",
    "    '''\n",
    "    对文本进行预处理\n",
    "    \n",
    "    参数\n",
    "    ---\n",
    "    text: 文本数据\n",
    "    freq: 词频阈值\n",
    "    '''\n",
    "    # 对文本中的符号进行替换\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # 删除低频词，减少噪音影响\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > freq]\n",
    "\n",
    "    return trimmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
     ]
    }
   ],
   "source": [
    "# 清洗文本并分词\n",
    "words = preprocess(text)\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建映射表\n",
    "vocab = set(words)\n",
    "vocab_to_int = {w: c for c, w in enumerate(vocab)}\n",
    "int_to_vocab = {c: w for c, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 16680599\n",
      "unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "print(\"total words: {}\".format(len(words)))\n",
    "print(\"unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对原文本进行vocab到int的转换\n",
    "int_words = [vocab_to_int[w] for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 采样\n",
    "\n",
    "对停用词进行采样，例如“the”， “of”以及“for”这类单词进行剔除。剔除这些单词以后能够加快我们的训练过程，同时减少训练过程中的噪音。\n",
    "\n",
    "我们采用以下公式:\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "其中$ t $是一个阈值参数，一般为1e-3至1e-5。  \n",
    "$f(w_i)$ 是单词 $w_i$ 在整个数据集中的出现频次。  \n",
    "$P(w_i)$ 是单词被删除的概率。\n",
    "\n",
    ">这个公式和论文中描述的那个公式有一些不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1e-5 # t值\n",
    "threshold = 0.8 # 剔除概率阈值\n",
    "\n",
    "# 统计单词出现频次\n",
    "int_word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "# 计算单词频率\n",
    "word_freqs = {w: c/total_count for w, c in int_word_counts.items()}\n",
    "# 计算被删除的概率\n",
    "prob_drop = {w: 1 - np.sqrt(t / word_freqs[w]) for w in int_word_counts}\n",
    "# 对单词进行采样\n",
    "train_words = [w for w in int_words if prob_drop[w] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6925252"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从上面数据可以看到，我们本身有1670万的文本，经过采样后剩下693万。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 构造batch\n",
    "\n",
    "Skip-Gram模型是通过输入词来预测上下文。因此我们要构造我们的训练样本，具体思想请参考知乎专栏，这里不再重复。\n",
    "\n",
    "对于一个给定词，离它越近的词可能与它越相关，离它越远的词越不相关，这里我们设置窗口大小为5，对于每个训练单词，我们还会在[1:5]之间随机生成一个整数R，用R作为我们最终选择output word的窗口大小。这里之所以多加了一步随机数的窗口重新选择步骤，是为了能够让模型更聚焦于当前input word的邻近词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, idx, window_size=5):\n",
    "    '''\n",
    "    获得input word的上下文单词列表\n",
    "    \n",
    "    参数\n",
    "    ---\n",
    "    words: 单词列表\n",
    "    idx: input word的索引号\n",
    "    window_size: 窗口大小\n",
    "    '''\n",
    "    target_window = np.random.randint(1, window_size+1)\n",
    "    # 这里要考虑input word前面单词不够的情况\n",
    "    start_point = idx - target_window if (idx - target_window) > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    # output words(即窗口中的上下文单词)\n",
    "    targets = set(words[start_point: idx] + words[idx+1: end_point+1])\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    '''\n",
    "    构造一个获取batch的生成器\n",
    "    '''\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # 仅取full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx: idx+batch_size]\n",
    "        for i in range(len(batch)):\n",
    "            batch_x = batch[i]\n",
    "            batch_y = get_targets(batch, i, window_size)\n",
    "            # 由于一个input word会对应多个output word，因此需要长度统一\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "            y.extend(batch_y)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 构建网络\n",
    "\n",
    "该部分主要包括：\n",
    "\n",
    "- 输入层\n",
    "- Embedding\n",
    "- Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "嵌入矩阵的矩阵形状为 $ vocab\\_size\\times hidden\\_units\\_size$ \n",
    "\n",
    "TensorFlow中的tf.nn.embedding_lookup函数可以实现lookup的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(int_to_vocab)\n",
    "embedding_size = 200 # 嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    # 嵌入层权重矩阵\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    # 实现lookup\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "负采样主要是为了解决梯度下降计算速度慢的问题，详情同样参考我的上一篇知乎专栏文章。\n",
    "\n",
    "TensorFlow中的tf.nn.sampled_softmax_loss会在softmax层上进行采样计算损失，计算出的loss要比full softmax loss低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    # 计算negative sampling下的损失\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证\n",
    "\n",
    "为了更加直观的看到我们训练的结果，我们将查看训练出的相近语义的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    # 随机挑选一些单词\n",
    "    valid_size = 16 \n",
    "    valid_window = 100\n",
    "    # 从不同位置各选8个单词\n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    \n",
    "    valid_size = len(valid_examples)\n",
    "    # 验证单词集\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # 计算每个词向量的模并进行单位化\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    # 查找验证单词的词向量\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    # 计算余弦相似度\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 100 Avg. Training loss: 3.6988 0.1283 sec/batch\n",
      "Epoch 1/10 Iteration: 200 Avg. Training loss: 3.6069 0.1244 sec/batch\n",
      "Epoch 1/10 Iteration: 300 Avg. Training loss: 3.6130 0.1230 sec/batch\n",
      "Epoch 1/10 Iteration: 400 Avg. Training loss: 3.5909 0.1233 sec/batch\n",
      "Epoch 1/10 Iteration: 500 Avg. Training loss: 3.5525 0.1226 sec/batch\n",
      "Epoch 1/10 Iteration: 600 Avg. Training loss: 3.5058 0.1221 sec/batch\n",
      "Epoch 1/10 Iteration: 700 Avg. Training loss: 3.4612 0.1219 sec/batch\n",
      "Epoch 1/10 Iteration: 800 Avg. Training loss: 3.4322 0.1220 sec/batch\n",
      "Epoch 1/10 Iteration: 900 Avg. Training loss: 3.3772 0.1220 sec/batch\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 3.2885 0.1213 sec/batch\n",
      "Nearest to [balcony]: trel, loveline, edgy, facts, topmost, hardwicke, rica, ballet,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, caller, sides, diab, isp, healing,\n",
      "Nearest to [dike]: weisstein, exile, stunning, dimitris, gunther, colombians, treatises, arenas,\n",
      "Nearest to [str]: punters, synods, achaean, bunghole, parr, trouser, messalina, publicised,\n",
      "Nearest to [anomaly]: hunts, shack, suny, ile, reinterpreting, kyrie, urea, fouling,\n",
      "Nearest to [hasler]: italics, illustrate, relena, laberge, ilyushin, andes, bottleneck, molar,\n",
      "Nearest to [machinations]: warwick, iy, neustria, toru, retrospectively, macmanus, despot, gwat,\n",
      "Nearest to [elective]: jvm, paraphrasing, kilometer, trick, cordillera, comprehensive, northerly, shutdown,\n",
      "Nearest to [deposit]: theoreticians, chlorate, election, navarrese, rida, sato, cycles, savona,\n",
      "Nearest to [thrashing]: lfheim, separates, malpractice, ichat, alderney, tribune, unrequited, pigweed,\n",
      "Nearest to [rehnquist]: defeating, gap, redress, prevented, wawel, bakesperson, hominidae, davao,\n",
      "Nearest to [lika]: falsity, semantic, pelicans, ilyas, modis, decrees, cripps, critica,\n",
      "Nearest to [rich]: port, rode, dist, mistral, ghanaian, sl, cognomen, tangent,\n",
      "Nearest to [specialization]: csc, theetu, plague, yawkey, expound, pane, colossians, sem,\n",
      "Nearest to [intelsat]: railway, everest, disintegrates, sightedness, cognomen, places, frenchy, wife,\n",
      "Nearest to [rohr]: lugdunum, cmavo, cantona, predatory, izz, workhouse, discontinuity, prozac,\n",
      "Epoch 1/10 Iteration: 1100 Avg. Training loss: 3.1847 0.1221 sec/batch\n",
      "Epoch 1/10 Iteration: 1200 Avg. Training loss: 3.0828 0.1222 sec/batch\n",
      "Epoch 1/10 Iteration: 1300 Avg. Training loss: 2.9563 0.1223 sec/batch\n",
      "Epoch 1/10 Iteration: 1400 Avg. Training loss: 2.9127 0.1224 sec/batch\n",
      "Epoch 1/10 Iteration: 1500 Avg. Training loss: 2.8312 0.1220 sec/batch\n",
      "Epoch 1/10 Iteration: 1600 Avg. Training loss: 2.7495 0.1219 sec/batch\n",
      "Epoch 1/10 Iteration: 1700 Avg. Training loss: 2.5942 0.1213 sec/batch\n",
      "Epoch 1/10 Iteration: 1800 Avg. Training loss: 2.6936 0.1216 sec/batch\n",
      "Epoch 1/10 Iteration: 1900 Avg. Training loss: 2.5636 0.1217 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 2.6697 0.1210 sec/batch\n",
      "Nearest to [balcony]: facts, edgy, rica, trel, loveline, topmost, hardwicke, citric,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, caller, sides, diab, propan, isp,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, treatises, gunther, colombians, arenas,\n",
      "Nearest to [str]: punters, achaean, synods, bunghole, parr, publicised, kato, messalina,\n",
      "Nearest to [anomaly]: hunts, shack, suny, reinterpreting, ile, blue, militaristic, collusion,\n",
      "Nearest to [hasler]: italics, illustrate, relena, ilyushin, andes, laberge, bottleneck, cyanobacterium,\n",
      "Nearest to [machinations]: warwick, iy, neustria, macmanus, retrospectively, savarkar, despot, toru,\n",
      "Nearest to [elective]: jvm, kilometer, comprehensive, trick, paraphrasing, cordillera, northerly, amherst,\n",
      "Nearest to [deposit]: theoreticians, election, cycles, savona, navarrese, slowed, chlorate, libraries,\n",
      "Nearest to [thrashing]: lfheim, separates, malpractice, ichat, tribune, pigweed, appendix, unrequited,\n",
      "Nearest to [rehnquist]: defeating, redress, gap, wawel, bakesperson, prevented, davao, hominidae,\n",
      "Nearest to [lika]: modis, semantic, decrees, falsity, rioting, ilyas, cripps, pelicans,\n",
      "Nearest to [rich]: port, flourished, rode, restored, strokes, designated, dedham, continuity,\n",
      "Nearest to [specialization]: csc, expound, pane, plague, consulars, theetu, colossians, sem,\n",
      "Nearest to [intelsat]: railway, everest, sightedness, highways, barajas, wife, suffice, cognomen,\n",
      "Nearest to [rohr]: lugdunum, cmavo, cantona, predatory, izz, discontinuity, prozac, milken,\n",
      "Epoch 1/10 Iteration: 2100 Avg. Training loss: 2.5940 0.1225 sec/batch\n",
      "Epoch 1/10 Iteration: 2200 Avg. Training loss: 2.5898 0.1215 sec/batch\n",
      "Epoch 1/10 Iteration: 2300 Avg. Training loss: 2.6114 0.1215 sec/batch\n",
      "Epoch 1/10 Iteration: 2400 Avg. Training loss: 2.5311 0.1218 sec/batch\n",
      "Epoch 1/10 Iteration: 2500 Avg. Training loss: 2.6686 0.1216 sec/batch\n",
      "Epoch 1/10 Iteration: 2600 Avg. Training loss: 2.5576 0.1214 sec/batch\n",
      "Epoch 1/10 Iteration: 2700 Avg. Training loss: 2.5270 0.1212 sec/batch\n",
      "Epoch 1/10 Iteration: 2800 Avg. Training loss: 2.5544 0.1207 sec/batch\n",
      "Epoch 1/10 Iteration: 2900 Avg. Training loss: 2.6083 0.1212 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 2.6426 0.1225 sec/batch\n",
      "Nearest to [balcony]: facts, rica, edgy, topmost, trel, hardwicke, cruelty, citric,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, sides, caller, isp, diab, propan,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, gunther, treatises, colombians, arenas,\n",
      "Nearest to [str]: achaean, punters, synods, parr, bunghole, kato, publicised, messalina,\n",
      "Nearest to [anomaly]: hunts, shack, ile, suny, militaristic, reinterpreting, sula, blue,\n",
      "Nearest to [hasler]: italics, illustrate, bottleneck, relena, ilyushin, andes, laberge, molar,\n",
      "Nearest to [machinations]: warwick, iy, neustria, savarkar, retrospectively, macmanus, despot, toru,\n",
      "Nearest to [elective]: jvm, kilometer, trick, paraphrasing, northerly, comprehensive, forecasting, amherst,\n",
      "Nearest to [deposit]: theoreticians, chlorate, savona, election, navarrese, cycles, libraries, classmates,\n",
      "Nearest to [thrashing]: lfheim, separates, malpractice, ichat, appendix, pigweed, tribune, unrequited,\n",
      "Nearest to [rehnquist]: defeating, gap, redress, bakesperson, prevented, wawel, davao, strabismus,\n",
      "Nearest to [lika]: modis, decrees, semantic, rioting, falsity, ilyas, cripps, pelicans,\n",
      "Nearest to [rich]: port, rode, thread, dedham, continuity, sl, sultans, flourished,\n",
      "Nearest to [specialization]: csc, plague, expound, true, pane, consulars, emphasis, immigrants,\n",
      "Nearest to [intelsat]: everest, railway, sightedness, frenchy, suffice, disintegrates, barajas, cognomen,\n",
      "Nearest to [rohr]: lugdunum, cmavo, cantona, izz, discontinuity, predatory, prozac, milken,\n",
      "Epoch 1/10 Iteration: 3100 Avg. Training loss: 2.4590 0.1226 sec/batch\n",
      "Epoch 1/10 Iteration: 3200 Avg. Training loss: 2.6566 0.1214 sec/batch\n",
      "Epoch 1/10 Iteration: 3300 Avg. Training loss: 2.5588 0.1213 sec/batch\n",
      "Epoch 1/10 Iteration: 3400 Avg. Training loss: 2.5104 0.1209 sec/batch\n",
      "Epoch 1/10 Iteration: 3500 Avg. Training loss: 2.4965 0.1205 sec/batch\n",
      "Epoch 1/10 Iteration: 3600 Avg. Training loss: 2.5068 0.1211 sec/batch\n",
      "Epoch 1/10 Iteration: 3700 Avg. Training loss: 2.4594 0.1205 sec/batch\n",
      "Epoch 1/10 Iteration: 3800 Avg. Training loss: 2.5814 0.1206 sec/batch\n",
      "Epoch 1/10 Iteration: 3900 Avg. Training loss: 2.5270 0.1206 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 2.4009 0.1209 sec/batch\n",
      "Nearest to [balcony]: trel, rica, facts, loveline, topmost, edgy, hardwicke, unip,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, sides, caller, isp, diab, propan,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, gunther, treatises, colombians, arenas,\n",
      "Nearest to [str]: achaean, punters, synods, parr, bunghole, drums, aristides, kato,\n",
      "Nearest to [anomaly]: hunts, shack, ile, reinterpreting, blue, suny, mss, militaristic,\n",
      "Nearest to [hasler]: italics, illustrate, andes, bottleneck, relena, molar, ilyushin, laberge,\n",
      "Nearest to [machinations]: warwick, iy, neustria, savarkar, despot, macmanus, losses, retrospectively,\n",
      "Nearest to [elective]: jvm, kilometer, paraphrasing, trick, northerly, comprehensive, forecasting, shutdown,\n",
      "Nearest to [deposit]: chlorate, theoreticians, navarrese, savona, cycles, classmates, sato, rida,\n",
      "Nearest to [thrashing]: lfheim, separates, malpractice, ichat, appendix, tribune, pigweed, ernie,\n",
      "Nearest to [rehnquist]: defeating, redress, gap, prevented, bakesperson, wawel, davao, nobel,\n",
      "Nearest to [lika]: modis, decrees, falsity, rioting, ilyas, semantic, cripps, critica,\n",
      "Nearest to [rich]: rode, port, flourished, ghanaian, continuity, penned, cognomen, mistral,\n",
      "Nearest to [specialization]: csc, expound, pane, plague, colossians, true, harkonnen, consulars,\n",
      "Nearest to [intelsat]: everest, railway, barajas, disintegrates, frenchy, sightedness, lavender, analogs,\n",
      "Nearest to [rohr]: lugdunum, cmavo, cantona, izz, predatory, discontinuity, prozac, haigh,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 4100 Avg. Training loss: 2.4879 0.1220 sec/batch\n",
      "Epoch 1/10 Iteration: 4200 Avg. Training loss: 2.4661 0.1205 sec/batch\n",
      "Epoch 1/10 Iteration: 4300 Avg. Training loss: 2.4998 0.1208 sec/batch\n",
      "Epoch 1/10 Iteration: 4400 Avg. Training loss: 2.4872 0.1208 sec/batch\n",
      "Epoch 1/10 Iteration: 4500 Avg. Training loss: 2.5827 0.1205 sec/batch\n",
      "Epoch 1/10 Iteration: 4600 Avg. Training loss: 2.4817 0.1209 sec/batch\n",
      "Epoch 1/10 Iteration: 4700 Avg. Training loss: 2.6068 0.1215 sec/batch\n",
      "Epoch 1/10 Iteration: 4800 Avg. Training loss: 2.4749 0.1214 sec/batch\n",
      "Epoch 1/10 Iteration: 4900 Avg. Training loss: 2.4805 0.1215 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 2.3621 0.1214 sec/batch\n",
      "Nearest to [balcony]: facts, rica, trel, topmost, edgy, hardwicke, loveline, ilo,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, caller, isp, jacoby, sides, diab,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, gunther, treatises, colombians, peggy,\n",
      "Nearest to [str]: punters, achaean, synods, parr, bunghole, aristides, messalina, drums,\n",
      "Nearest to [anomaly]: shack, hunts, ile, sula, reinterpreting, collusion, mss, blue,\n",
      "Nearest to [hasler]: italics, illustrate, andes, bottleneck, relena, ilyushin, molar, intrinsically,\n",
      "Nearest to [machinations]: warwick, iy, neustria, savarkar, despot, macmanus, retrospectively, beneath,\n",
      "Nearest to [elective]: jvm, kilometer, paraphrasing, northerly, trick, comprehensive, fracture, shutdown,\n",
      "Nearest to [deposit]: chlorate, theoreticians, navarrese, classmates, savona, sato, handel, cycles,\n",
      "Nearest to [thrashing]: lfheim, malpractice, separates, ichat, tribune, appendix, pigweed, ernie,\n",
      "Nearest to [rehnquist]: defeating, redress, gap, prevented, bakesperson, wawel, davao, conjoined,\n",
      "Nearest to [lika]: modis, decrees, falsity, rioting, semantic, ilyas, cripps, critica,\n",
      "Nearest to [rich]: port, rode, cognomen, ghanaian, backstage, strokes, penned, rubies,\n",
      "Nearest to [specialization]: csc, plague, expound, colossians, pane, consulars, true, sanitation,\n",
      "Nearest to [intelsat]: everest, railway, barajas, frenchy, disintegrates, highways, sightedness, lavender,\n",
      "Nearest to [rohr]: lugdunum, cmavo, cantona, izz, predatory, discontinuity, haigh, salina,\n",
      "Epoch 1/10 Iteration: 5100 Avg. Training loss: 2.3400 0.1216 sec/batch\n",
      "Epoch 1/10 Iteration: 5200 Avg. Training loss: 2.4785 0.1200 sec/batch\n",
      "Epoch 1/10 Iteration: 5300 Avg. Training loss: 2.4974 0.1211 sec/batch\n",
      "Epoch 1/10 Iteration: 5400 Avg. Training loss: 2.5649 0.1207 sec/batch\n",
      "Epoch 1/10 Iteration: 5500 Avg. Training loss: 2.5152 0.1213 sec/batch\n",
      "Epoch 1/10 Iteration: 5600 Avg. Training loss: 2.4608 0.1202 sec/batch\n",
      "Epoch 1/10 Iteration: 5700 Avg. Training loss: 2.6520 0.1196 sec/batch\n",
      "Epoch 1/10 Iteration: 5800 Avg. Training loss: 2.5303 0.1208 sec/batch\n",
      "Epoch 1/10 Iteration: 5900 Avg. Training loss: 2.3117 0.1202 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 2.5770 0.1198 sec/batch\n",
      "Nearest to [balcony]: facts, rica, hardwicke, topmost, edgy, trel, loveline, camillo,\n",
      "Nearest to [tiberias]: michel, osgood, jacoby, ogden, caller, isp, sides, vojvodina,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, gunther, treatises, colombians, peggy,\n",
      "Nearest to [str]: punters, achaean, synods, parr, bunghole, aristides, neodymium, archbishop,\n",
      "Nearest to [anomaly]: shack, hunts, ile, sula, reinterpreting, blue, fouling, collusion,\n",
      "Nearest to [hasler]: italics, illustrate, andes, bottleneck, ilyushin, relena, molar, cyanobacterium,\n",
      "Nearest to [machinations]: warwick, iy, neustria, despot, retrospectively, savarkar, losses, macmanus,\n",
      "Nearest to [elective]: kilometer, jvm, paraphrasing, northerly, forecasting, trick, comprehensive, fracture,\n",
      "Nearest to [deposit]: chlorate, navarrese, theoreticians, savona, classmates, cycles, handel, rida,\n",
      "Nearest to [thrashing]: lfheim, malpractice, separates, ichat, tribune, pigweed, appendix, unrequited,\n",
      "Nearest to [rehnquist]: defeating, redress, gap, prevented, wawel, bakesperson, nobel, conjoined,\n",
      "Nearest to [lika]: falsity, modis, rioting, ilyas, decrees, semantic, cripps, critica,\n",
      "Nearest to [rich]: port, rode, rubies, backstage, cognomen, penned, dist, strokes,\n",
      "Nearest to [specialization]: csc, plague, colossians, expound, pane, consulars, sem, theetu,\n",
      "Nearest to [intelsat]: everest, railway, ifrcs, stations, highways, barajas, confederates, frenchy,\n",
      "Nearest to [rohr]: cmavo, cantona, lugdunum, izz, predatory, discontinuity, haigh, milken,\n",
      "Epoch 1/10 Iteration: 6100 Avg. Training loss: 2.5518 0.1214 sec/batch\n",
      "Epoch 1/10 Iteration: 6200 Avg. Training loss: 2.3605 0.1201 sec/batch\n",
      "Epoch 1/10 Iteration: 6300 Avg. Training loss: 2.3758 0.1201 sec/batch\n",
      "Epoch 1/10 Iteration: 6400 Avg. Training loss: 2.4316 0.1206 sec/batch\n",
      "Epoch 1/10 Iteration: 6500 Avg. Training loss: 2.4221 0.1208 sec/batch\n",
      "Epoch 1/10 Iteration: 6600 Avg. Training loss: 2.4443 0.1212 sec/batch\n",
      "Epoch 1/10 Iteration: 6700 Avg. Training loss: 2.5123 0.1207 sec/batch\n",
      "Epoch 1/10 Iteration: 6800 Avg. Training loss: 2.5056 0.1208 sec/batch\n",
      "Epoch 1/10 Iteration: 6900 Avg. Training loss: 2.5503 0.1208 sec/batch\n",
      "Epoch 2/10 Iteration: 7000 Avg. Training loss: 2.3535 0.0910 sec/batch\n",
      "Nearest to [balcony]: facts, rica, trel, edgy, unip, loveline, hardwicke, cruelty,\n",
      "Nearest to [tiberias]: michel, osgood, jacoby, ogden, caller, isp, vojvodina, diab,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, treatises, gunther, colombians, peggy,\n",
      "Nearest to [str]: punters, achaean, synods, bunghole, parr, aristides, archbishop, drums,\n",
      "Nearest to [anomaly]: shack, hunts, ile, sula, blue, fouling, mss, reinterpreting,\n",
      "Nearest to [hasler]: italics, illustrate, andes, bottleneck, ilyushin, relena, laberge, molar,\n",
      "Nearest to [machinations]: warwick, iy, neustria, losses, retrospectively, macmanus, despot, savarkar,\n",
      "Nearest to [elective]: kilometer, jvm, paraphrasing, downtown, vcd, amherst, fracture, comprehensive,\n",
      "Nearest to [deposit]: chlorate, navarrese, theoreticians, savona, classmates, cycles, randle, handel,\n",
      "Nearest to [thrashing]: lfheim, malpractice, tribune, ichat, separates, appendix, pigweed, unrequited,\n",
      "Nearest to [rehnquist]: defeating, redress, prevented, gap, bakesperson, wawel, writeup, davao,\n",
      "Nearest to [lika]: falsity, rioting, modis, ilyas, decrees, critica, semantic, pelicans,\n",
      "Nearest to [rich]: rubies, dedham, escalators, backstage, adrift, flourished, cognomen, port,\n",
      "Nearest to [specialization]: csc, expound, plague, colossians, pane, consulars, sem, vicarage,\n",
      "Nearest to [intelsat]: everest, railway, stations, ifrcs, barajas, highways, disintegrates, utilities,\n",
      "Nearest to [rohr]: cmavo, lugdunum, cantona, izz, predatory, discontinuity, milken, haigh,\n",
      "Epoch 2/10 Iteration: 7100 Avg. Training loss: 2.3203 0.1227 sec/batch\n",
      "Epoch 2/10 Iteration: 7200 Avg. Training loss: 2.3471 0.1214 sec/batch\n",
      "Epoch 2/10 Iteration: 7300 Avg. Training loss: 2.3578 0.1215 sec/batch\n",
      "Epoch 2/10 Iteration: 7400 Avg. Training loss: 2.3909 0.1228 sec/batch\n",
      "Epoch 2/10 Iteration: 7500 Avg. Training loss: 2.3571 0.1225 sec/batch\n",
      "Epoch 2/10 Iteration: 7600 Avg. Training loss: 2.2725 0.1221 sec/batch\n",
      "Epoch 2/10 Iteration: 7700 Avg. Training loss: 2.3552 0.1210 sec/batch\n",
      "Epoch 2/10 Iteration: 7800 Avg. Training loss: 2.2385 0.1213 sec/batch\n",
      "Epoch 2/10 Iteration: 7900 Avg. Training loss: 2.3578 0.1216 sec/batch\n",
      "Epoch 2/10 Iteration: 8000 Avg. Training loss: 2.1856 0.1213 sec/batch\n",
      "Nearest to [balcony]: facts, trel, rica, loveline, edgy, unip, topmost, cruelty,\n",
      "Nearest to [tiberias]: michel, osgood, jacoby, ogden, caller, diab, isp, vojvodina,\n",
      "Nearest to [dike]: weisstein, stunning, exile, dimitris, gunther, treatises, colombians, peggy,\n",
      "Nearest to [str]: punters, achaean, synods, bunghole, parr, aristides, ara, messalina,\n",
      "Nearest to [anomaly]: shack, hunts, ile, fouling, blue, sula, mss, reinterpreting,\n",
      "Nearest to [hasler]: italics, illustrate, andes, bottleneck, relena, ilyushin, laberge, molar,\n",
      "Nearest to [machinations]: warwick, iy, neustria, macmanus, losses, savarkar, retrospectively, diplomatically,\n",
      "Nearest to [elective]: kilometer, jvm, paraphrasing, airport, downtown, northerly, vcd, amherst,\n",
      "Nearest to [deposit]: chlorate, navarrese, theoreticians, savona, cycles, handel, randle, classmates,\n",
      "Nearest to [thrashing]: lfheim, malpractice, ichat, tribune, separates, pigweed, appendix, unrequited,\n",
      "Nearest to [rehnquist]: defeating, redress, gap, prevented, davao, wawel, writeup, bakesperson,\n",
      "Nearest to [lika]: falsity, rioting, modis, ilyas, decrees, critica, pelicans, semantic,\n",
      "Nearest to [rich]: rubies, escalators, dedham, backstage, cognomen, flourished, penned, adrift,\n",
      "Nearest to [specialization]: csc, expound, colossians, plague, pane, consulars, sem, theetu,\n",
      "Nearest to [intelsat]: everest, railway, stations, ifrcs, barajas, highways, ifc, utilities,\n",
      "Nearest to [rohr]: cmavo, lugdunum, cantona, izz, predatory, milken, ri, discontinuity,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Iteration: 8100 Avg. Training loss: 2.2695 0.1234 sec/batch\n",
      "Epoch 2/10 Iteration: 8200 Avg. Training loss: 2.3289 0.1217 sec/batch\n",
      "Epoch 2/10 Iteration: 8300 Avg. Training loss: 2.3567 0.1214 sec/batch\n",
      "Epoch 2/10 Iteration: 8400 Avg. Training loss: 2.3214 0.1216 sec/batch\n",
      "Epoch 2/10 Iteration: 8500 Avg. Training loss: 2.3521 0.1215 sec/batch\n",
      "Epoch 2/10 Iteration: 8600 Avg. Training loss: 2.1754 0.1220 sec/batch\n",
      "Epoch 2/10 Iteration: 8700 Avg. Training loss: 2.0903 0.1212 sec/batch\n",
      "Epoch 2/10 Iteration: 8800 Avg. Training loss: 2.3257 0.1207 sec/batch\n",
      "Epoch 2/10 Iteration: 8900 Avg. Training loss: 2.1662 0.1211 sec/batch\n",
      "Epoch 2/10 Iteration: 9000 Avg. Training loss: 2.3168 0.1207 sec/batch\n",
      "Nearest to [balcony]: trel, facts, rica, loveline, edgy, unip, duquette, topmost,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, jacoby, caller, diab, isp, vojvodina,\n",
      "Nearest to [dike]: weisstein, stunning, exile, gunther, dimitris, colombians, treatises, peggy,\n",
      "Nearest to [str]: punters, achaean, synods, bunghole, parr, messalina, aristides, neodymium,\n",
      "Nearest to [anomaly]: shack, hunts, ile, blue, sula, mss, fouling, urea,\n",
      "Nearest to [hasler]: italics, illustrate, andes, bottleneck, ilyushin, relena, laberge, intrinsically,\n",
      "Nearest to [machinations]: warwick, iy, neustria, macmanus, losses, savarkar, retrospectively, despot,\n",
      "Nearest to [elective]: kilometer, jvm, paraphrasing, airport, downtown, vcd, northerly, comprehensive,\n",
      "Nearest to [deposit]: chlorate, navarrese, theoreticians, savona, cycles, slowed, handel, rida,\n",
      "Nearest to [thrashing]: lfheim, ichat, appendix, malpractice, pigweed, tribune, unrequited, separates,\n",
      "Nearest to [rehnquist]: defeating, redress, prevented, writeup, davao, wawel, gap, bakesperson,\n",
      "Nearest to [lika]: decrees, falsity, rioting, modis, ilyas, pie, critica, pelicans,\n",
      "Nearest to [rich]: escalators, rubies, backstage, dedham, flourished, penned, cognomen, adrift,\n",
      "Nearest to [specialization]: csc, expound, plague, colossians, pane, sanitation, sem, telegraphic,\n",
      "Nearest to [intelsat]: everest, railway, stations, ifrcs, ifc, ioc, barajas, utilities,\n",
      "Nearest to [rohr]: cmavo, lugdunum, cantona, izz, predatory, milken, kaaba, discontinuity,\n",
      "Epoch 2/10 Iteration: 9100 Avg. Training loss: 2.3060 0.1230 sec/batch\n",
      "Epoch 2/10 Iteration: 9200 Avg. Training loss: 2.3377 0.1213 sec/batch\n",
      "Epoch 2/10 Iteration: 9300 Avg. Training loss: 2.2467 0.1215 sec/batch\n",
      "Epoch 2/10 Iteration: 9400 Avg. Training loss: 2.2853 0.1217 sec/batch\n",
      "Epoch 2/10 Iteration: 9500 Avg. Training loss: 2.2185 0.1214 sec/batch\n",
      "Epoch 2/10 Iteration: 9600 Avg. Training loss: 2.1398 0.1211 sec/batch\n",
      "Epoch 2/10 Iteration: 9700 Avg. Training loss: 2.2416 0.1203 sec/batch\n",
      "Epoch 2/10 Iteration: 9800 Avg. Training loss: 2.2914 0.1209 sec/batch\n",
      "Epoch 2/10 Iteration: 9900 Avg. Training loss: 2.2921 0.1211 sec/batch\n",
      "Epoch 2/10 Iteration: 10000 Avg. Training loss: 2.1750 0.1218 sec/batch\n",
      "Nearest to [balcony]: trel, loveline, rica, facts, unip, edgy, topmost, duquette,\n",
      "Nearest to [tiberias]: michel, osgood, ogden, jacoby, caller, diab, robards, vojvodina,\n",
      "Nearest to [dike]: weisstein, stunning, exile, gunther, dimitris, treatises, colombians, arenas,\n",
      "Nearest to [str]: punters, achaean, synods, bunghole, parr, messalina, aristides, neodymium,\n",
      "Nearest to [anomaly]: shack, hunts, blue, sula, urea, ile, fouling, mss,\n",
      "Nearest to [hasler]: italics, illustrate, andes, relena, ilyushin, laberge, molar, hitchcock,\n",
      "Nearest to [machinations]: warwick, iy, neustria, macmanus, losses, savarkar, despot, retrospectively,\n",
      "Nearest to [elective]: kilometer, jvm, paraphrasing, vcd, downtown, northerly, comprehensive, amherst,\n",
      "Nearest to [deposit]: chlorate, navarrese, cycles, theoreticians, savona, slowed, handel, rida,\n",
      "Nearest to [thrashing]: lfheim, tribune, ichat, malpractice, pigweed, appendix, unrequited, separates,\n",
      "Nearest to [rehnquist]: defeating, redress, wawel, davao, prevented, bakesperson, writeup, gap,\n",
      "Nearest to [lika]: decrees, falsity, rioting, modis, ilyas, pie, critica, pelicans,\n",
      "Nearest to [rich]: dedham, escalators, penned, rubies, flourished, adrift, phidias, axial,\n",
      "Nearest to [specialization]: csc, expound, plague, colossians, sanitation, sem, pane, consulars,\n",
      "Nearest to [intelsat]: everest, railway, ifrcs, stations, ifc, ioc, opcw, barajas,\n",
      "Nearest to [rohr]: cmavo, lugdunum, cantona, izz, predatory, milken, kaaba, ri,\n",
      "Epoch 2/10 Iteration: 10100 Avg. Training loss: 2.2179 0.1224 sec/batch\n",
      "Epoch 2/10 Iteration: 10200 Avg. Training loss: 2.2246 0.1209 sec/batch\n",
      "Epoch 2/10 Iteration: 10300 Avg. Training loss: 2.2703 0.1207 sec/batch\n",
      "Epoch 2/10 Iteration: 10400 Avg. Training loss: 2.2085 0.1200 sec/batch\n",
      "Epoch 2/10 Iteration: 10500 Avg. Training loss: 2.3401 0.1208 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # 迭代轮数\n",
    "batch_size = 1000 # batch大小\n",
    "window_size = 10 # 窗口大小\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver() # 文件存储\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        # \n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            # 计算相似的词\n",
    "            if iteration % 1000 == 0:\n",
    "                # 计算similarity\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # 取最相似单词的前8个\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to [%s]:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
